@inproceedings{7,
  author    = {Anthony Stentz and Dieter Fox and Michael Montemerlo and Michael Montemerlo},
  title     = {FastSLAM: A Factored Solution to the Simultaneous Localization and Mapping Problem with Unknown Data Association},
  booktitle = {In Proceedings of the AAAI National Conference on Artificial Intelligence},
  year      = {2003},
  pages     = {593--598},
  publisher = {AAAI}
}

@inproceedings{20,
  author    = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Value Prediction Network},
  url       = {https://proceedings.neurips.cc/paper/2017/file/ffbd6cbb019a1413183c8d08f2929307-Paper.pdf},
  volume    = {30},
  year      = {2017}
}

@inproceedings{22,
  author    = {Manderson, Travis and Wapnick, Stefan and Meger, David and Dudek, Gregory},
  booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {1263-1269},
  doi       = {10.1109/ICRA40945.2020.9196879}
}

@inproceedings{26,
  author    = {Ho, Jonathan and Ermon, Stefano},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Generative Adversarial Imitation Learning},
  url       = {https://proceedings.neurips.cc/paper/2016/file/cc7e2b878868cbae992d1fb743995d8f-Paper.pdf},
  volume    = {29},
  year      = {2016}
}

@inproceedings{38,
  author    = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
  booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Are we ready for autonomous driving? The KITTI vision benchmark suite},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {3354-3361},
  doi       = {10.1109/CVPR.2012.6248074}
}

@article{39,
  title         = {Vision meets Robotics: The {KITTI} Dataset},
  author        = {Geiger, Andreas and Lenz, Philip and Stiller, Christoph and Urtasun, Raquel},
  journal       = {International Journal of Robotics Research},
  volume        = {32},
  number        = {11},
  pages         = {1231 - 1237 },
  publisher     = {Sage Publishing},
  month         = sep,
  year          = {2013},
  doi           = {10.1177/0278364913491297 },
  month_numeric = {9}
}

@inproceedings{DA,
  author    = {Triess, Larissa T. and Dreissig, Mariella and Rist, Christoph B. and Marius ZÃ¶llner, J.},
  booktitle = {2021 IEEE Intelligent Vehicles Symposium Workshops (IV Workshops)},
  title     = {A Survey on Deep Domain Adaptation for LiDAR Perception},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {350-357},
  doi       = {10.1109/IVWorkshops54471.2021.9669228}
}

@inproceedings{CARLA,
  title     = { {CARLA}: {An} Open Urban Driving Simulator},
  author    = {Alexey Dosovitskiy and German Ros and Felipe Codevilla and Antonio Lopez and Vladlen Koltun},
  booktitle = {Proceedings of the 1st Annual Conference on Robot Learning},
  pages     = {1--16},
  year      = {2017}
}

@article{CBAM,
  title    = {CBAM: Convolutional Block Attention Module},
  author   = { Woo, S.  and  Park, J.  and  Lee, J. Y.  and  Kweon, I. S. },
  journal  = {Springer, Cham},
  year     = {2018},
  abstract = {We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.}
}

@inproceedings{37,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {770-778},
  doi       = {10.1109/CVPR.2016.90}
}

@inproceedings{36,
  author    = {Charles, R. Qi and Su, Hao and Kaichun, Mo and Guibas, Leonidas J.},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {77-85},
  doi       = {10.1109/CVPR.2017.16}
}

@inproceedings{35,
  title     = {RangeNet ++: Fast and Accurate LiDAR Semantic Segmentation},
  author    = { Milioto, A.  and  Vizzo, I.  and  Behley, J.  and  Stachniss, C. },
  booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year      = {2019},
  abstract  = {Perception in autonomous vehicles is often carried out through a suite of different sensing modalities. Given the massive amount of openly available labeled RGB data and the advent of high-quality deep learning algorithms for image-based recognition, high-level semantic perception tasks are pre-dominantly solved using high-resolution cameras. As a result of that, other sensor modalities potentially useful for this task are often ignored. In this paper, we push the state of the art in LiDAR-only semantic segmentation forward in order to provide another independent source of semantic information to the vehicle. Our approach can accurately perform full semantic segmentation of LiDAR point clouds at sensor frame rate. We exploit range images as an intermediate representation in combination with a Convolutional Neural Network (CNN) exploiting the rotating LiDAR sensor model. To obtain accurate results, we propose a novel post-processing algorithm that deals with problems arising from this intermediate representation such as discretization errors and blurry CNN outputs. We implemented and thoroughly evaluated our approach including several comparisons to the state of the art. Our experiments show that our approach outperforms state-of-the-art approaches, while still running online on a single embedded GPU. The code can be accessed at https://github.com/PRBonn/lidar-bonnetal.}
}

@inproceedings{34,
  title     = {Multi-View 3D Object Detection Network for Autonomous Driving},
  author    = { Chen, X.  and  Ma, H.  and  Wan, J.  and  Li, B.  and  Xia, T. },
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2017},
  abstract  = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the birds eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 14.9% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.}
}

@article{33,
  author  = {Li, Xiang and Li, Jun and Hu, Xiaolin and Yang, Jian},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  title   = {Line-CNN: End-to-End Traffic Line Detection With Line Proposal Unit},
  year    = {2020},
  volume  = {21},
  number  = {1},
  pages   = {248-258},
  doi     = {10.1109/TITS.2019.2890870}
}

@article{32,
  title    = {DeepGoal: Learning to drive with driving intention from human control demonstration - ScienceDirect},
  author   = { Hm, A  and  Yue, W. A.  and  Rong, X. A.  and  Sk, B  and  Li, T. A. },
  journal  = {Robotics and Autonomous Systems},
  volume   = {127},
  abstract = {Recent research on automotive driving has developed an efficient end-to-end learning mode that directly maps visual input to control commands. However, it models distinct driving variations in a single network, which increases learning complexity and is less adaptive for modular integration. In this paper, we re-investigate human's driving style and propose to learn an intermediate driving intention region to relax the difficulties in end-to-end approach. The intention region follows both road structure in image and direction towards goal in public route planner, which addresses visual variations only and figures out where to go without conventional precise localization. Then the learned visual intention is projected on vehicle local coordinate and fused with reliable obstacle perception to render a navigation score map that is widely used for motion planning. The core of the proposed system is a weakly-supervised cGAN-LSTM model trained to learn driving intention from human demonstration. The adversarial loss learns from limited demonstration data with one local planned route and enables reasoning of multi-modal behaviors with diverse routes while testing. Comprehensive experiments are conducted with real-world datasets. Results indicate the proposed paradigm can produce more consistent motion commands with human demonstration and shows better reliability and robustness to environment change. Our code is available at https://github.com/HuifangZJU/visual-navigation.}
}


@inproceedings{31,
  title     = {Conditional Affordance Learning for Driving in Urban Environments},
  author    = {Sauer, Axel and Savinov, Nikolay and Geiger, Andreas},
  booktitle = {Proceedings of The 2nd Conference on Robot Learning},
  pages     = {237--252},
  year      = {2018},
  editor    = {Billard, Aude and Dragan, Anca and Peters, Jan and Morimoto, Jun},
  volume    = {87},
  series    = {Proceedings of Machine Learning Research},
  month     = {29--31 Oct},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v87/sauer18a/sauer18a.pdf},
  url       = {https://proceedings.mlr.press/v87/sauer18a.html},
  abstract  = {Most existing approaches to autonomous driving fall into one of two categories: modular pipelines, that build an extensive model of the environment, and imitation learning approaches, that map images directly to control outputs. A recently proposed third paradigm, direct perception, aims to combine the advantages of both by using a neural network to learn appropriate low-dimensional intermediate representations. However, existing direct perception approaches are restricted to simple highway situations, lacking the ability to navigate intersections, stop at traffic lights or respect speed limits. In this work, we propose a direct perception approach which maps video input to intermediate representations suitable for autonomous navigation in complex urban environments given high-level directional inputs. Compared to state-of-the-art reinforcement and conditional imitation learning approaches, we achieve an improvement of up to 68 % in goal-directed navigation on the challenging CARLA simulation benchmark. In addition, our approach is the first to handle traffic lights and speed signs by using image-level labels only, as well as smooth car-following, resulting in a significant reduction of traffic accidents in simulation. }
}


@inproceedings{30,
  title     = {Deep learning algorithm for autonomous driving using GoogLeNet},
  author    = { Al-Qizwini, M.  and  Barjasteh, I.  and  Al-Qassab, H.  and  Radha, H. },
  booktitle = {2017 IEEE Intelligent Vehicles Symposium (IV)},
  year      = {2017},
  abstract  = {In this paper, we consider the Direct Perception approach for autonomous driving. Previous efforts in this field focused more on feature extraction of the road markings and other vehicles in the scene rather than on the autonomous driving algorithm and its performance under realistic assumptions. Our main contribution in this paper is introducing a new, more robust, and more realistic Direct Perception framework and corresponding algorithm for autonomous driving. First, we compare the top 3 Convolutional Neural Networks (CNN) models in the feature extraction competitions and test their performance for autonomous driving. The experimental results showed that GoogLeNet performs the best in this application. Subsequently, we propose a deep learning based algorithm for autonomous driving, and we refer to our algorithm as GoogLenet for Autonomous Driving (GLAD). Unlike previous efforts, GLAD makes no unrealistic assumptions about the autonomous vehicle or its surroundings, and it uses only five affordance parameters to control the vehicle as compared to the 14 parameters used by prior efforts. Our simulation results show that the proposed GLAD algorithm outperforms previous Direct Perception algorithms both on empty roads and while driving with other surrounding vehicles.}
}

@article{29,
  title    = {DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving},
  author   = { Chen, C.  and  Seff, A.  and  Kornhauser, A.  and  Xiao, J. },
  journal  = {IEEE},
  pages    = {2722-2730},
  year     = {2015},
  abstract = {Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. In this paper, we propose a third paradigm: a direct perception approach to estimate the affordance for driving. We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously. Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction. To demonstrate this, we train a deep Convolutional Neural Network using recording from 12 hours of human driving in a video game and show that our model can work well to drive a car in a very diverse set of virtual environments. We also train a model for car distance estimation on the KITTI dataset. Results show that our direct perception approach can generalize well to real driving images. Source code and data are available on our project website.}
}

@article{28,
  title    = {A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning},
  author   = { Ross, S.  and  Gordon, G. J.  and  Bagnell, J. A. },
  journal  = {Aistats},
  volume   = {abs/1011.0686},
  year     = {2011},
  abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.}
}

@article{27,
  title    = {A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots},
  author   = { Giusti, A.  and  Guzzi, J.  and  Ciresan, D.  and  He, F. L.  and  Rodriguez, J. P.  and  Fontana, F.  and  Faessler, M.  and  Forster, C.  and  Schmidhuber, J.  and  Caro, G. D. },
  journal  = {IEEE Robotics and Automation Letters},
  pages    = {1-1},
  year     = {2017},
  abstract = {We study the problem of perceiving forest or mountain trails from a single monocular image acquired from the viewpoint of a robot traveling on the trail itself. Previous literature focused on trail segmentation, and used low-level features such as image saliency or appearance contrast; we propose a different approach based on a deep neural network used as a supervised image classifier. By operating on the whole image at once, our system outputs the main direction of the trail compared to the viewing direction. Qualitative and quantitative results computed on a large real-world dataset (which we provide for download) show that our approach outperforms alternatives, and yields an accuracy comparable to the accuracy of humans that are tested on the same image classification task. Preliminary results on using this information for quadrotor control in unseen trails are reported. To the best of our knowledge, this is the first letter that describes an approach to perceive forest trials, which is demonstrated on a quadrotor micro aerial vehicle.}
}

@article{25,
  title    = {End to End Learning for Self-Driving Cars},
  author   = { Bojarski, M.  and  Testa, D Del  and  Dworakowski, D.  and  Firner, B.  and  Flepp, B.  and  Goyal, P.  and  Jackel, L. D.  and  Monfort, M.  and  Muller, U.  and  Zhang, J. },
  year     = {2016},
  abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).}
}

@inproceedings{24,
  title     = {DART: Noise Injection for Robust Imitation Learning},
  author    = { Laskey, M.  and  Lee, J.  and  Fox, R.  and  Dragan, A.  and  Goldberg, K. },
  booktitle = {1s Conference on Robot Learning},
  year      = {2017},
  abstract  = {For applications such as Amazon warehouse order fulfillment, robots must grasp a desired object amid clutter: other objects that block direct access. This can be difficult to program explicitly due to uncertainty in friction and push mechanics and the variety of objects that can be encountered. Deep Learning networks combined with Online Learning from Demonstration (LfD) algorithms such as...}
}

@article{23,
  title    = {ALVINN : An autonomous land vehicle in a neural network},
  author   = { Pomerleau, D. },
  journal  = {Advances in Neural Information Processing Systems},
  volume   = {1},
  year     = {1989},
  abstract = {No abstract provided.}
}

@inproceedings{21,
  author    = {Kahn, Gregory and Villaflor, Adam and Ding, Bosen and Abbeel, Pieter and Levine, Sergey},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Self-Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {5129-5136},
  doi       = {10.1109/ICRA.2018.8460655}
}

@article{19,
  title    = {Safe Nonlinear Trajectory Generation for Parallel Autonomy With a Dynamic Vehicle Model},
  author   = { Schwarting, W.  and  Alonso-Mora, J.  and  Paull, L.  and  Karaman, S.  and D Rus},
  journal  = {IEEE Transactions on Intelligent Transportation Systems},
  pages    = {2994-3008},
  year     = {2017},
  abstract = {High-end vehicles are already equipped with safety systems, such as assistive braking and automatic lane following, enhancing vehicle safety. Yet, these current solutions can only help in low-complexity driving situations. In this paper, we introduce a parallel autonomy, or shared control, framework that computes safe trajectories for an automated vehicle, based on human inputs. We minimize the deviation from the human inputs while ensuring safety via a set of collision avoidance constraints. Our method achieves safe motion even in complex driving scenarios, such as those commonly encountered in an urban setting. We introduce a receding horizon planner formulated as nonlinear model predictive control (NMPC), which includes the analytic descriptions of road boundaries and the configuration and future uncertainties of other road participants. The NMPC operates over both steering and acceleration simultaneously. We introduce a nonslip model suitable for handling complex environments with dynamic obstacles, and a nonlinear combined slip vehicle model including normal load transfer capable of handling static environments. We validate the proposed approach in two complex driving scenarios. First, in an urban environment that includes a left-turn across traffic and passing on a busy street. And second, under snow conditions on a race track with sharp turns and under complex dynamic constraints. We evaluate the performance of the method with various human driving styles. We consequently observe that the method successfully avoids collisions and generates motions with minimal intervention for parallel autonomy. We note that the method can also be applied to generate safe motion for fully autonomous vehicles.}
}

@article{18,
  title    = {Predictive Active Steering Control for Autonomous Vehicle Systems},
  author   = { Falcone, P.  and  Borrelli, F.  and  Asgari, J.  and  Tseng, H. E.  and  Hrovat, D. },
  journal  = {IEEE Transactions on Control Systems Technology},
  volume   = {15},
  number   = {3},
  pages    = {566-580},
  year     = {2007},
  abstract = {In this paper, a model predictive control (MPC) approach for controlling an active front steering system in an autonomous vehicle is presented. At each time step, a trajectory is assumed to be known over a finite horizon, and an MPC controller computes the front steering angle in order to follow the trajectory on slippery roads at the highest possible entry speed. We present two approaches with different computational complexities. In the first approach, we formulate the MPC problem by using a nonlinear vehicle model. The second approach is based on successive online linearization of the vehicle model. Discussions on computational complexity and performance of the two schemes are presented. The effectiveness of the proposed MPC formulation is demonstrated by simulation and experimental tests up to 21 m/s on icy roads}
}

@inproceedings{17,
  title  = {Rapidly-Exploring Random Trees: Progress and Prospects},
  author = {Steven M. LaValle and James J. Kuffner},
  year   = {2000}
}

@article{16,
  title    = {Probabilistic Roadmaps for Path Planning in High-Dimensional Configuration Spaces},
  author   = { Kavraki, L. E.  and  Svestka, P.  and  Latombe, J. C.  and  Overmars, M. H. },
  journal  = {IEEE Transactions on Robotics and Automation},
  volume   = {12},
  number   = {4},
  pages    = {566-580},
  year     = {1996},
  abstract = {date-added={2012-09-25 10:06:31 +0200}, date-modified={2012-09-25 10:06:31 +0200}, project={fremdliteratur}}
}

@inproceedings{15,
  title     = {Anytime Motion Planning using the RRT*},
  author    = { Karaman, S.  and  Walter, M. R.  and  Perez, A.  and  Frazzoli, E.  and  Teller, S. J. },
  booktitle = {2011 IEEE International Conference on Robotics and Automation},
  year      = {2011},
  abstract  = {A desirable property of path planning for robotic manipulation is the ability to identify solutions in a sufficiently short amount of time to be usable. This is particularly challenging for the manipulation problem due to the need to plan over high-dimensional configuration spaces and to perform computationally expensive collision checking procedures. Consequently, existing planners take steps...}
}

@article{14,
  title    = {YOLOv3: An Incremental Improvement},
  author   = { Redmon, J.  and  Farhadi, A. },
  journal  = {arXiv e-prints},
  year     = {2018},
  abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at this https URL}
}

@article{13,
  title    = {SSD: Single Shot MultiBox Detector},
  author   = { Liu, W.  and  Anguelov, D.  and  Erhan, D.  and  Szegedy, C.  and  Reed, S.  and  Fu, C. Y.  and  Berg, A. C. },
  journal  = {Springer, Cham},
  year     = {2016},
  abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over...}
}

@inproceedings{12,
  author    = {Lang, Alex H. and Vora, Sourabh and Caesar, Holger and Zhou, Lubing and Yang, Jiong and Beijbom, Oscar},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {PointPillars: Fast Encoders for Object Detection From Point Clouds},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {12689-12697},
  doi       = {10.1109/CVPR.2019.01298}
}

@article{11,
  title    = {DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs},
  author   = { Chen, L. C.  and  Papandreou, G  and  Kokkinos, I  and  Murphy, K  and  Yuille, A. L. },
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume   = {40},
  number   = {4},
  pages    = {834-848},
  year     = {2018},
  abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrousconvolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrousspatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.}
}

@article{10,
  title    = {3D-SSD: Learning hierarchical features from RGB-D images for amodal 3D object detection - ScienceDirect},
  author   = { Luo, Q.  and  Ma, H.  and  Tang, L.  and  Wang, Y.  and  Xiong, R. },
  journal  = {Neurocomputing},
  volume   = {378},
  pages    = {364-374},
  year     = {2020},
  abstract = {This paper aims at developing a faster and more accurate solution to the amodal 3D object detection problem for indoor scenarios. The solution is achieved through a novel neural network structure which takes a pair of RGB-D images as input and delivers oriented 3D bounding boxes as the output. Such network, named 3D-SSD, has two components: hierarchical feature fusion and multi-layer prediction. The hierarchical feature fusion combines multi-scale appearance and geometric features learned from RGB-D images, which is later utilized in the multi-layer prediction for object detection. Both the accuracy and the efficiency can be improved by exploiting 2.5D representations in a synergistic way. To specifically address the shape variance of different objects, a set of 3D anchor boxes with varying physical sizes are attached to every location on the prediction layers. While testing, the category scores for 3D anchor boxes are generated with adjusted positions, sizes and orientations, leading to the final detections using non-maximum suppression. Comprehensive experiments have been performed on publicly accessible dataset of SUN RGB-D and NYUV2. The results show the proposed algorithm is the first 3D detector that runs in near real-time on the challenging datasets with competitive performance to the state-of-the-art methods. The 3D-SSD gets 37.1% mAP on the SUN RGB-D dataset at around 5.6 fps, which outperforms the state-of-the-art Deep Sliding Shape by 10.2% mAP and around 109 Ã faster. For an efficient model setting with a rate of 9.3 fps, 3D-SSD still gets an accuracy of 37% on mAP. Further, experiments also suggest the proposed approach achieves comparable accuracy and is about 477 Ã faster than the state-of-art method on the NYUv2 dataset even with a smaller input image size.}
}

@inproceedings{9,
  title     = {Simultaneous Map Building and Localization for an Autonomous Mobile Robot},
  author    = { Leonard, J. J.  and  Durrant-Whyte, H. F. },
  booktitle = {Intelligent Robots and Systems '91. 'Intelligence for Mechanical Systems, Proceedings IROS '91. IEEE/RSJ International Workshop on},
  year      = {1991},
  abstract  = {Discusses a significant open problem in mobile robotics: simultaneous map building and localization, which the authors define as long-term globally referenced position estimation without a priori information. This problem is difficult because of the following paradox: to move precisely, a mobile robot must have an accurate environment map; however, to build an accurate map, the mobile robot's sensing locations must be known precisely. In this way, simultaneous map building and localization can be seen to present a question of 'which came first, the chicken or the egg?' (The map or the motion?) When using ultrasonic sensing, to overcome this issue the authors equip the vehicle with multiple servo-mounted sonar sensors, to provide a means in which a subset of environment features can be precisely learned from the robot's initial location and subsequently tracked to provide precise positioning.>}
}

@article{8,
  title    = {MonoSLAM: real-time single camera SLAM},
  author   = { Davison, A. J.  and  Reid, I. D.  and  Molton, N. D.  and  Stasse, O. },
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume   = {29},
  number   = {6},
  pages    = {1052-1067},
  year     = {2007},
  abstract = {We present a real-time algorithm which can recover the 3D trajectory of a monocular camera, moving rapidly through a previously unknown scene. Our system, which we dub MonoSLAM, is the first successful application of the SLAM methodology from mobile robotics to the "pure vision" domain of a single uncontrolled camera, achieving real time but drift-free performance inaccessible to Structure from Motion approaches. The core of the approach is the online creation of a sparse but persistent map of natural landmarks within a probabilistic framework. Our key novel contributions include an active approach to mapping and measurement, the use of a general motion model for smooth camera movement, and solutions for monocular feature initialization and feature orientation estimation. Together, these add up to an extremely efficient and robust algorithm which runs at 30 Hz with standard PC and camera hardware. This work extends the range of robotic systems in which SLAM can be usefully applied, but also opens up new areas. We present applications of MonoSLAM to real-time 3D localization and mapping for a high-performance full-size humanoid robot and live augmented reality with a hand-held camera.}
}

@article{6,
  title    = {BADGR: An Autonomous Self-Supervised Learning-Based Navigation System},
  author   = { Kahn, G.  and  Abbeel, P.  and  Levine, S. },
  journal  = {IEEE Robotics and Automation Letters},
  volume   = {PP},
  number   = {99},
  pages    = {1-1},
  year     = {2021},
  abstract = {Mobile robot navigation is typically regarded as a geometric problem, in which the robot's objective is to perceive the geometry of the environment in order to plan collision-free paths towards a desired goal. However, a purely geometric view of the world can be insufficient for many navigation problems. For example, a robot navigating based on geometry may avoid a field of tall grass because it believes it is untraversable, and will therefore fail to reach its desired goal. In this work, we investigate how to move beyond these purely geometric-based approaches using a method that learns about physical navigational affordances from experience. Our reinforcement learning approach, which we call BADGR, is an end-to-end learning-based mobile robot navigation system that can be trained with self-supervised off-policy data gathered in real-world environments, without any simulation or human supervision. BADGR can navigate in real-world urban and off-road environments with geometrically distracting obstacles. It can also incorporate terrain preferences, generalize to novel environments, and continue to improve autonomously by gathering more data. Videos, code, and other supplemental material are available on our website https://sites.google.com/view/badgr}
}

@article{5,
  title    = {Variational End-to-End Navigation and Localization},
  author   = { Amini, A.  and  Rosman, G.  and  Karaman, S.  and  Rus, D. },
  journal  = {arXiv},
  year     = {2019},
  abstract = {Deep learning has revolutionized the ability to learn "end-to-end" autonomous vehicle control directly from raw sensory data. While there have been recent advances on extensions to handle forms of navigation instruction, these works are unable to capture the full distribution of possible actions that could be taken and to reason about localization of the robot within the environment. In this paper, we extend end-to-end driving networks with the ability to understand maps. We define a novel variational network capable of learning from raw camera data of the environment as well as higher level roadmaps to predict (1) a full probability distribution over the possible control commands; and (2) a deterministic control command capable of navigating on the route specified within the map. Additionally, we formulate how our model can be used to localize the robot according to correspondences between the map and the observed visual road topology, inspired by the rough localization that human drivers can perform. We evaluate our algorithms on real-world driving data, and reason about the robustness of the inferred steering commands under various types of rich driving scenarios. In addition, we evaluate our localization algorithm over a new set of roads and intersections which the model has never driven through and demonstrate rough localization in situations without any GPS prior.}
}

@article{4,
  title    = {Learning Robust Control Policies for End-to-End Autonomous Driving From Data-Driven Simulation},
  author   = { Amini, A.  and  Gilitschenski, I.  and  Phillips, J.  and  Moseyko, J.  and  Rus, D. },
  journal  = {IEEE Robotics and Automation Letters},
  volume   = {5},
  number   = {2},
  pages    = {1-1},
  year     = {2020},
  abstract = {In this work, we present a data-driven simulation and training engine capable of learning end-to-end autonomous vehicle control policies using only sparse rewards. By leveraging real, human-collected trajectories through an environment, we render novel training data that allows virtual agents to drive along a continuum of new local trajectories consistent with the road appearance and semantics, each with a different view of the scene. We demonstrate the ability of policies learned within our simulator to generalize to and navigate in previously unseen real-world roads, without access to any human control labels during training. Our results validate the learned policy onboard a full-scale autonomous vehicle, including in previously un-encountered scenarios, such as new roads and novel, complex, near-crash situations. Our methods are scalable, leverage reinforcement learning, and apply broadly to situations requiring effective perception and robust operation in the physical world.}
}

@article{3,
  title    = {End-to-End Learning of Driving Models with Surround-View Cameras and Route Planners},
  author   = { Hecker, S.  and  Dai, D.  and  Gool, L. V. },
  journal  = {Springer, Cham},
  year     = {2018},
  abstract = {For human drivers, having rear and side-view mirrors is vital for safe driving. They deliver a more complete view of what is happening around the car. Human drivers also heavily exploit their mental...}
}

@inproceedings{2,
  author    = {Xu, Huazhe and Gao, Yang and Yu, Fisher and Darrell, Trevor},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {End-to-End Learning of Driving Models from Large-Scale Video Datasets},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {3530-3538},
  doi       = {10.1109/CVPR.2017.376}
}

@inproceedings{1,
  author    = {Codevilla, Felipe and MÃ¼ller, Matthias and LÃ³pez, Antonio and Koltun, Vladlen and Dosovitskiy, Alexey},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {End-to-End Driving Via Conditional Imitation Learning},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {4693-4700},
  doi       = {10.1109/ICRA.2018.8460487}
}

@misc{tensorflow2015-whitepaper,
  title  = { {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  url    = {https://www.tensorflow.org/},
  note   = {Software available from tensorflow.org},
  author = {
            Mart\'{\i}n~Abadi and
            Ashish~Agarwal and
            Paul~Barham and
            Eugene~Brevdo and
            Zhifeng~Chen and
            Craig~Citro and
            Greg~S.~Corrado and
            Andy~Davis and
            Jeffrey~Dean and
            Matthieu~Devin and
            Sanjay~Ghemawat and
            Ian~Goodfellow and
            Andrew~Harp and
            Geoffrey~Irving and
            Michael~Isard and
            Yangqing Jia and
            Rafal~Jozefowicz and
            Lukasz~Kaiser and
            Manjunath~Kudlur and
            Josh~Levenberg and
            Dandelion~Man\'{e} and
            Rajat~Monga and
            Sherry~Moore and
            Derek~Murray and
            Chris~Olah and
            Mike~Schuster and
            Jonathon~Shlens and
            Benoit~Steiner and
            Ilya~Sutskever and
            Kunal~Talwar and
            Paul~Tucker and
            Vincent~Vanhoucke and
            Vijay~Vasudevan and
            Fernanda~Vi\'{e}gas and
            Oriol~Vinyals and
            Pete~Warden and
            Martin~Wattenberg and
            Martin~Wicke and
            Yuan~Yu and
            Xiaoqiang~Zheng},
  year   = {2015}
}

@inproceedings{bn,
  author    = {Ioffe, Sergey and Szegedy, Christian},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  year      = {2015},
  publisher = {JMLR.org},
  abstract  = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
  booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
  pages     = {448â456},
  numpages  = {9},
  location  = {Lille, France},
  series    = {ICML'15}
}

@inproceedings{GND,
  author    = {Paigwar, Anshul and Erkent, ÃzgÃ¼r and Sierra-Gonzalez, David and Laugier, Christian},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {GndNet: Fast Ground Plane Estimation and Point Cloud Segmentation for Autonomous Vehicles},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {2150-2156},
  doi       = {10.1109/IROS45743.2020.9340979}
}